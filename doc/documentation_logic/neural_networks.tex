%------------------------------------------------------
%Author             : Jonathan Schwarz
%University         : Pforzheim University
%Date of last edit  : Fri, 06 Mar 2015 10:00:53 +0100
%Filename           : neural_networks.tex
%------------------------------------------------------

\documentclass[10pt,a4paper,DIV=11]{scrreprt}

%British English
\usepackage[UKenglish]{babel}
\usepackage[tbtags]{amsmath}
%utf8
\usepackage[utf8]{inputenc}

%pseudo-code
\usepackage[boxruled,vlined]{algorithm2e}

\usepackage{scrhack}
%for source code listings
\usepackage{listings}
%tables
\usepackage[table]{xcolor}
%figures alongside tables
\usepackage{floatrow}
\newfloatcommand{capbtabbox}{table}[][]

%tikz
\usepackage{tikz,pgf}
\usetikzlibrary{arrows, positioning, shapes, fit}
%\usepackage[top=1in,bottom=1in,right=1in,left=1in]{geometry}
\usepackage{pgf-pie}
\usepackage[font=small,labelfont=bf,tableposition=top]{caption}
\DeclareCaptionLabelFormat{andtable}{#1~#2  \&  \tablename~\thetable}

%male and female symbol
\usepackage{wasysym}
%plots
\usepackage{pgfplots}

%blocks - used by tikz-uml, included before
\pgfdeclarelayer{background}
\pgfdeclarelayer{foreground}
\pgfsetlayers{background,main,foreground}

%<,> in tikz-uml
\usepackage[T1]{fontenc}
% \usepackage{tikz-uml}

%subfigure
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

%prevent figure from floating pictures
\usepackage{float}

%footer & header
\usepackage{fancyhdr}

%push footer down
\usepackage[bottom]{footmisc}

%footer & header
\pagestyle{fancy}
%clean footer & header
\fancyhf{}

%acronyms
\usepackage[nohyperlinks]{acronym}

%bibtex
\usepackage[square,numbers]{natbib}
\usepackage{gensymb}

%maths
\usepackage{amssymb} 
\let\oldemptyset\emptyset
\let\emptyset\varnothing
\usepackage{dsfont}
\usepackage{mathtools}

%japanese
\usepackage{CJKutf8}

%table of contents with hyperlinks
%always include as last package
\usepackage{hyperref}

%===========================TITLE PAGE=======================================

%university logo
\titlehead
{
    \includegraphics[width=0.20\textwidth]{files/hspflogo.pdf}\\

    Pforzheim University\\
    School of Engineering\\
}

\subject{Project work}
	
\title
{
     A comparison of neural network types and learning techniques with an application in artificial life.
}

\author
{
    \textbf{Jonathan Schwarz} - matriculation number: 304727
}
\date
{
    Summer term 2015
}
%\today{}}

\publishers
{
    Examiner: Prof. Dr. Richard Alznauer\\
    Supervisor: Dr. Christoph Ußfeller
}


%=========================================GLOBAL SETTINGS=========================================

%footer &header

%\fancyfoot[L]{\textbf{Multi-Threading mit POSIX-pThreads}}
\fancyhead[R]{Page \thepage}
%\fancyhead[L]{\thechapter}

%chapter number and title
\fancyhead[L]{\nouppercase{\leftmark}}
%line
%\renewcommand{\footrulewidth}{0.5 pt}
\usepackage{lmodern}
\addtokomafont{sectioning}{\rmfamily}
\setlength{\parindent}{0mm}

%colour definitions
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{grey}{rgb}{0.7,0.7,0.7}
%medium grey
\definecolor{mgrey}{gray}{0.80}
%light grey
\definecolor{lgrey}{gray}{0.97}

%hyperlink settings
%frame around hyperlinks
\hypersetup
{
    colorlinks = false,
    linkcolor = black,
    hypertexnames = false,
    citecolor = green
}

%listing settings
\lstset
{ 
    language=C,                
    basicstyle=\footnotesize\ttfamily,           
    numbers=left,
    stepnumber=5,    
    firstnumber=1,
    numberfirstline=true                 
    numberstyle=\color{black},                 
    numbersep=5pt,                 
    backgroundcolor=\color{white},      
    showspaces=false,             
    showstringspaces=false,         
    showtabs=false,                
    frame=single,                   
    rulecolor=\color{black},       
    tabsize=2,                     
    captionpos=b,                   
    breaklines=true,                
    breakatwhitespace=false,       
    title=\lstname,                    
    keywordstyle=\color{blue},          
    commentstyle=\color{dkgreen}, 
    identifierstyle=\color{black},      
    stringstyle=\color{purple},      
    escapeinside={\%*}{*)},      
    morekeywords={*,...},            
    deletekeywords={...}             
}

\newenvironment{Japanese}{%
    \CJKfamily{min}%
    \CJKtilde
\CJKnospace}{}

\setcounter{tocdepth}{4}  %a deeper contents menue
%=====================================DOCUMENT START=========================
\begin{document}

\tikzstyle{line}=[draw]
\tikzstyle{arrow}=[draw, -latex] 

%\renewcommand*\contentsname{Content}
%\renewcommand*\listtablename{Tables}
%\renewcommand*\listfigurename{Figures}
%\renewcommand*\bibname{Literature references}

\maketitle
\thispagestyle{empty}
\newpage
{\large\tableofcontents}
\newpage

\thispagestyle{empty}

\section*{List of acronyms}
\begin{acronym}
    \acro{$ANN$}{Artificial neural network}
    \acro{$a_j$}{Activity of unit $n_j$}
    \acro{$a_j(t)$}{Activity of unit $n_j$ at time $t$}
    \acro{$H$}{The set of hidden layers of a neural network}
    \acro{$L$}{The set of layers of a neural network}
    \acro{$MLN$}{A multi-layer network}
    \acro{$n_j$}{Neuron (also unit) $j$}
    \acro{$W$}{The connection matrix}
%    \acro{weight}[$w_{ij}$}{Weight on the connection link between neurons $n_i$ and $n_j$}
    \acro{deltaweight}[$\Delta w_{ij}$]{Weight change on the connection link between unit $i$ and $j$}
    \acro{ivec}[$\protect \overrightarrow{x}$]{The input vector of a neural network}
    \acro{ovec}[$\protect \overrightarrow{y}$]{The output vector of a neural network}
    \acro{act}[$\varphi$]{Activation (or transfer) function of a neuron}
    \acro{eta}[$\eta$]{Learning rate of a network}
    \acro{grad}[$\nabla F(a)$]{Gradient of the multivariate function $F(x)$ at point $a$}
\end{acronym}

\newpage

\chapter{Introduction}
\label{ch:intro}
In \cite{DANIEL}, Schembri proposes the application of machine learning algorithms for training of object collecting agents. 
Such agents dwell in a simulated, confined, two-dimensional space, where objects are dropped at random. Without any previous knowledge of 
their environment, agents are placed in the world and are to reason that their goal is to collect objects. The performance or fitness of each 
individual is rated by the amount of objects collected in a given time period. Thus, this fitness ranking serves as a quality measure for the 
chosen approaches.


In particular, Schembri is interested in using artificial neural networks to devise a control policy for each agent at any point in time. 
Unsupervised learning with genetic algorithms is chosen as the means to achieve satisfactory behaviour. Its principle is to alter 
an agents behaviour and evaluate whether this amendment improved or lowered the agents performance. In case of a successful change, the changes
will be accepted, whereas they are reverted if the change was not. By gradual application of this technique, Schembri tries to increase
the performance so as to achieve intelligent behaviour.


This project builds off of aforementioned work and both extends the unsupervised approach and also shows that successful learning can be 
achieved by means of previous training. The basic rationale behind this supervised approach is that an agent is presented a set of correct 
policies from given input data to aspired motion. It is then to learn the underlying control policy itself and apply it to
previously unseen data. In addition to the learning approach, alterations in the neural network structure can also have significant influence
to the agents performance. Hence, different structures will be proposed and empirically evaluated.


Both approaches will be tested in the very same simulation environment as in the original paper. A critical evaluation of both learning approaches
in combination with different network structures is given.

\paragraph{Motivation}
Machine learning techniques power a variety of aspects of modern life. Content filtering, speech recognition, object detection in
images or product suggestions are examples of fields in which great progress has been made through its application. 
With the advent of representation learning, machines can now be fed with raw data and are able to extract the representation needed for detection
or classification. This is a major achievement as the successful application of Machine learning has previously taken careful engineering and
domain-specific knowledge. A set of methodologies called \textit{Deep learning} applies representation learning to multiple layers of representation. 
In image processing, such a layer may represent the presence of edges in an image. Deep learning has proven itself outstandingly useful in detecting 
patterns in high-dimensional data. In an age of availability of large datasets, this is interesting for many fields in science, business and government.
Besides the aforementioned technology examples, Deep learning was also used to reconstruct brain circuits or predict the effect of mutations in
non-coding DNA on gene expression and disease.\cite{DEEPLEARNING}


Thus, we regard neural networks (Which Deep learning is at a more abstract level) as a field much worth of study.

\paragraph{Structure of this documentation}
In chapter \ref{ch:basics} a brief introduction to the very basics of artificial neural networks is given. We investigate their biological counterpart
and show a mathematical formalisation of their structure. We dive more into the details of artificial neural networks in chapter \ref{ch:ANN} where
we different models and network types are introduced. Several learning algorithms for these structures are then presented in chapter \label{ch:learning}. 
The presented learning methods include both supervised as well as unsupervised approaches. Finally, we show how neural networks can be used to 
control an agent as proposed in Schembri's work in chapter \label{ch:design}. Evaluation of presented algorithms follows in chapter \label{ch:eval}, 
where we critically analyse the performance gain in comparison to Schembri's work. A final conclusion is drawn and ideas of further work are given
in chapter \label{ch:conclusion}.

\chapter{Basics}
\label{ch:basics}
\section{Neural networks}
The operation of artificial neural networks (ANNs) is based on and modeled after out understanding of the human brain. 
Our brain is an astonishingly powerful information processing system. It is capable of solving extremely complex tasks such as recognising a face, or
processing vision and audio signals simultaneously at ease. In contrast, programming a computer to do so turns out to be rather difficult.
A distinctive feature of artificial neural networks is their ability to learn and abstract from examples. As distinct from computers, 
ANNs do not have to be programmed to solve a certain task. This feature makes them particularly interesting for tasks where no solution or at 
least no efficient algorithm exist. Further examples of successful applications of neural networks are given in section \ref{sec:exp}.


In terms of computability, they are an equivalent model to the Turing machine.\cite{NURING}


Neural networks may be looked at from widely different angles. Their models serve neuroscience for the understanding of biological processes 
in the human brain, while a psychologist verifies learning- and memory theories using those models. A physicist regards them as a physical structure,
which can be described by their energy functions and probabilistic regularities. From a computer scientific point of view, neural networks
are massive parallelized, adaptive and partially self-organised information processing systems. \cite{NNGER}
Within this project, neural network (or rather artificial ones) are regarded as a means of machine learning to allow computer to learn and reason
from data. Machine learning itself may be understood as a sub field or means to an end of artificial intelligence.
We are particularly interested in using them to define a decision policy for intelligent agents. Such a decision policy uses knowledge of the agent's
environment, obtained by sensors, to manipulate it using its actuators. This is done in a way to maximise the expected future reward (collecting as
many objects as possible). Nevertheless, a brief explanation of their biological inspiration promotes the understanding of their structure and is 
therefore given in the following sections. In addition, we show how they have been described mathematically, which allows their representation 
in a computer program.

\section{Biological background}
From all the other organs, the human brain is of special significance. It is the only organ that does not process any metabolic products, but a 
‘substance’ which became the target of scientific investigation during the last century: Information. It was mainly through the works of 
\textit{Golgi} and \textit{Ramón y Cajal}, that science began to perceive it as a complex net of interconnected cells. Contemporary 
concepts of the operation of the brain suggests, that mental activity consists primarily of electrochemical activity in large networks of nerve 
cells, or neurons. Figure \ref{fig:neuron} introduces the different parts of the neuron. Branching out from the cell body or soma, several fibers 
called dendrites summarise the input of the cell body from up to 100,000 connected neurons. In case this electric potential exceeds a threshold, 
the cell body will produce a quick spike, which is transmitted by the axon. The axon is a single long fiber, typically with a length of 1 cm,
which is roughly 100 times the diameter of the cell body. As a matter of fact, it can a reach a length of up to several metres. As the axon branches 
out, it transmits this signal to other neurons. The synaptic terminals, are contacts of the axon. The electric potential of the signal causes the 
release of chemicals called neurotransmitter, which lead to a potential change. This change can be either excitatory or inhibitory, amplifying or 
reducing the signal. This signal amendment defines the behavior of the network on two levels: 

\begin{itemize}
    \item Brain activity in relatively short periods (within seconds), defined by the current activity state of each individual neuron. 
    The occurring activity patterns encode continuously changing data. Among others, both short term memory and immediate perception are believed to be 
represented by this changing activity pattern.
    \item The other level of change is defined by the interaction of the neurons between each other, e.g. by forming new connections. 
    Those changes happen a lot slower, taking minutes up to several days.\cite{NEUINF} 
\end{itemize}

\begin{center}
\begin{figure}[H]

\includegraphics[width=0.8\textwidth,scale=1]{files/neuron.jpg}  
\caption{The parts of a nerve cell or neuron as shown in \cite{NEU}.}
\label{fig:neuron}
\end{figure}
\end{center}


\section{Mathematical formalisation}
\label{sec:mcpitts}
During the second world war, research in computer technology has been promoted for the sake of military purposes. With the onset of computer 
technology, neural networks started to become significant as models of abstract automata. McCulloch and Pitt's 1943 published work 
(>>\textit{A logical calculus of the ideas immanent in nervous activity}<<)\cite{MCP} proposes a cell which allows the simulation of AND-, OR- and 
NOT-gates and therefore every boolean function. This cell is a simple model of a neuron with binary threshold and the first formal description of a 
neural network. Within their work, neurons are perceived as basic Input/Output units, which process input of $i$ neurons and propagate their 
output further into the network until a single neuron to the very right constitutes the output.\\

The activity level of a neuron $j$ will throughout this paper referred to as $a_j$. It assigned by its activation function $\varphi$. 
McCulloch and Pitt proposed a simple binary step function as activation function $\varphi$, where $S$ is the threshold:\\

\begin{equation}
	\varphi(x)=\begin{cases}
		0: \quad  x < S \\
		1: \quad  x \geq S \\
	\end{cases}
\label{eq:step}
\end{equation}
Given the definition of $\varphi$ in equation \eqref{eq:step}, activity levels are always binary and computed as follows:\\

\begin{equation}
a_j = \varphi(\sum_{i}^{} a_{i}), \quad a_i \in \{0, 1\}
\end{equation}

Figure \ref{fig:mathneuron} illustrates the mathematical formalisation proposed in \cite{NEURONMATH}. This model has later been 
named \textbf{McCulloch-Pitts neuron}. 

\begin{figure}[H]

\centering

\begin{tikzpicture}
\node [align=center] (in1) at (-0.5,0) {$a_i$}; 
\node [align=center] (in2) at (-0.3,0.75)   {}; 
\node [align=center] (in3) at (-0.8,1.5) {$a_i = 1$}; 
%\node [align=center] (w1) at (0.7,0) {$w_{ij}$}; 
%\node [align=center] (w3) at (0.7,1.5) {$w_{0j}$};
\node [align=center] (input) at (2,0.75) {\ \ $\sum_{}^{in_{j}}$}; 
\node [align=center] (function) at (4,0.75) {$\varphi$}; 
\node [align=center] (output) at (6,0.75) {$a_j$}; 
%\node [align=center] (o1) at (8.5,0) {};
\node [align=center] (o2) at (8.5,0.75) {};
%\node [align=center] (o3) at (8.5,1.5) {};

\node [align=center] (desilinks) at (-0.60,-1) {Input\\ Links}; 
\node [align=center] (desinput) at (1.9,-1) {Input\\ funcion}; 
\node [align=center] (desfunc) at (4,-1) {Activation\\ function}; 
\node [align=center] (desoutput) at (6,-1) {Output}; 
\node [align=center] (desolinks) at (8,-1) {Output\\ link}; 

\node [align=center] (desweight) at (4,2) {$a_j = \varphi(in_j)$}; 
%\node [align=center] (desinfunc) at (0.75,2) {\small Weight}; 
\node [align=center] (descneuron) at (6,1.5) {\small Cell body}; 

\node [align=center] (dummy1) at (2.4,0.75) {}; 
\node [align=center] (dummy2) at (5.36,0.75) {}; 

\draw[arrow]	(in1) -- (input);
\draw[arrow]	(in2) -- (input);
\draw[arrow]	(in3) -- (input);

%\draw[arrow]	(output) -- (o1);
\draw[arrow]	(output) -- (o2);
%\draw[arrow]	(output) -- (o3);

\node[draw=black, fit=(dummy1) (function) (dummy2), ellipse] (tmp) {};

\end{tikzpicture}
\caption{A mathematical model of a neuron $j$}
\label{fig:mathneuron}
\end{figure}

This proposal is of particular importance for the theory of ANNs and inspired significant researchers like John von Neumann or Nobert Wiener, 
who developed cybernetics. However, the McCulloch-Pitts neurons are limited by their inability to learn, resulting from their fixed structure. 
We will explain how this model can be extended to enable neural networks to learn in chapter \ref{ch:learning}.
They also lack the surprising robustness of biological networks, since they rely on a flawless operation of all components. Nevertheless, 
they still have significance in electrical engineering, where they are used to realise binary functions, due to their increased efficiency in 
contrast to gates. 

\chapter{Artificial neural networks}
\label{ch:ANN}
\section{Overview}

Scientists have proposed a multitude of different network structures since the advent of research in ANNs through their first formalisation. 
A lot of which have been designed with various motivations, stemming from the different perception of neural networks across disciplines.
Categorisation can be achieved by distinguishing several parameters, e.g. the number of layers, acceptance of input types or usage of learning techniques.
Figure \ref{fig:class} illustrates a possible categorisation taken from \cite{NNGER}. Note that although not all of the mentioned networks are shown in details,
we mention their existence so that an interested reader may be allowed to do further research on the topic.
The direction of signal flow serves as a first level of distinction, as it substantially characterises the networks structure, complexity and so on.
Both types of signal flow mandate the propagation of network input further into the network.
Feed-forward network prohibit the usage of non-straightforward signal flow, i.e. connections may only emerge between subsequent neurons.
Any connections to predecessors are forbidden. This constraint has allowed for a quick development of learning techniques, empowering ANNs by 
allowing them to learn from data. 


As distinct from feedforward networks, recurrent (or feedback) networks allow bidirectional signal flow. 
Links from one neuron are no longer retricted to lead to neurons in the next layer, but may be formed with any other neuron in the network.
Since the current state of a neuron may be continuously changing (e.g. when a connection is formed from and to itself), recurrent networks 
demand advanced learning algorithms and are an order of magnitude more complex in terms of structure. Due to their dynamic nature, they allow 
for the modelling of advanced techniques, such as long and short term memory (LSTM).\cite{LSTM}

\begin{figure}[H]

\centering

\fbox{
\begin{tikzpicture}
\node [align=center] (L1) at (3,10) {feedforward}; 

\node [align=center] (R1) at (10,10) {recurrent}; 


\node [align=center] (L21) at (2,8.5) {one layer,\\ binary}; 
\node [align=center] (L22) at (4,8.5) {multi layer}; 

\node [align=center] (R21) at (9,9) {deterministic}; 
\node [align=center] (R22) at (11,9) {stochastic}; 


\node [align=center] (L31) at (1,7) {Perceptron\\Adaline}; 
\node [align=center] (L32) at (3,7) {non-recurrent}; 
\node [align=center] (L33) at (5,7) {recurrent}; 

\node [align=center] (R31) at (8,7.75) {self organising}; 
\node [align=center] (R32) at (10.6,6.5) {BSB Hopfield\\BAM}; 
\node [align=center] (R33) at (12,7.75) {Simulated annealing/\\Boltzmann-Machine}; 


\node [align=center] (L41) at (2.25,6) {BPG}; 
\node [align=center] (L42) at (3.25,6) {CPN}; \node [align=center] (L43) at (4.35,6) {SRN}; 
\node [align=center] (L44) at (5.35,6) {LTSM}; 

\node [align=center] (R41) at (6.5,6) {ART}; 
\node [align=center] (R42) at (8.80,6) {Cognition/\\Neocognition}; 

\foreach \s/\t in {L1/L21,
                   L1/L22,
                   L21/L31,
                   L22/L32,
                   L22/L33,
                   L32/L41,
                   L32/L42,
                   L33/L43,
                   L33/L44}
    \draw (\s) -- (\t);

\foreach \s/\t in {R1/R21,
                   R1/R22,
                   R21/R31,
                   R21/R32,
                   R22/R33,
                   R31/R41,
                   R31/R42}
    \draw (\s) -- (\t);

\end{tikzpicture}
}
\caption{A categorisation of neural network types. BPG = Backpropagation, CPN = Counterpropagation, BAM = Bidirectional associative memory, 
BSB = Brain-State-in-a-Box, ART = Adaptive Resonance Theory, SRN = Simple recurrent networks, LSTM = Long short term memory}
\label{fig:class}
\end{figure}

A comprehensive introduction of all categorised network types would exceed the scope of this project work, and therefore only a subset,
interesting for the particular application, will be introduced. Within this chapter, we will give an introduction of the terminology and mathematical
models used to represent a neural network on a computing machine. An appraisal of advantages and detriments of the presented models is rather 
difficult, as their usefulness strongly depends on the particular application. Regarding usefulness or performance, machine learning, and therefore 
also ANNs, demand for a new definition as distinct from classical computer science:


\textit{“Machine learning is a scientific discipline that explores the construction and study of algorithms that can learn from data.”}\cite{MLDEF1}\\ 


\textit{“Such algorithms operate by building a model from example inputs and using that to make predictions or decisions.”} \cite{MLDEF2} 


Machine learning is closely related to statistics and many models associated with it build upon probabilistic modelling of data or express their 
results as a probability distribution over a set of random variables. As distinct from non-probabilistic algorithms, there can never be absolute 
certainty of the correctness of a machine learning model. This stems from their usage of probability theory and statistical modelling which are 
applied to real world problems. Thus, algorithmic measures, such as optimality and completeness, cannot be utilised to assess the usefulness of such a technique.
A performance evaluation can therefore only be conducted given a specific problem and the according data. After all, the selection and 
appropriate usage of machine learning algorithms (and therefore also neural networks) requires extensive experience and/or thorough testing. 

\section{Network structure}
\subsection{Layers}
The units within a network are categorised into several layers according to their position. This allows for an accurate description of the network 
and the function of each respective unit within the network. The set of layers of a ANN will be denoted as $L$, where an artificial neural network 
of size $n$, written as $\text{ANN}^n$ is the union of the $n$ layers in the network:

\begin{equation}
    \text{ANN}^n = \bigcup_{N}^{i=1}{L_i}
\end{equation}

The amount of neurons within a layer is equal to the elements of the respective set. The total number of neurons in an ANN is 
$\sum_{i=1}^{n}{|L_i|}$. The tuple $(|L_1|,...,|L_n|)$ is called the \textit{network topolgy} and describes the 
structure (with exclusion of the connections) of a feedforward network. Recurrent networks demand additional description.

The first network layer $L_1$, is referred to as the input layer. It receives the input vector $\overrightarrow{x} \in \mathds{R}^{|L_1|}$ and 
passes it onto the next layer. Likewise, $L_n$ is the output layer of the network, whose neurons are called the output units. 
$\overrightarrow{y} \in \mathds{R}^{|L_n|}$ is the output vector of an ANN. A computer application will later utilise the neural network as a black 
box, being only interested in the transformation from $\overrightarrow{x}$ to $\overrightarrow{y}$. Figure \ref{fig:layer} shows this. 

\begin{figure}[H]

\centering

\begin{tikzpicture}
\node [draw=black, align=center, circle] (i1) at (0,3) {}; 
\node [draw=black, align=center, circle] (i2) at (0,2) {}; 
\node [draw=black, align=center, circle] (i3) at (0,1) {}; 
\node [draw=black, align=center, circle] (i4) at (0,0) {}; 

\node [draw=black, align=center, circle] (h01) at (2,2.5) {}; 
\node [draw=black, align=center, circle] (h02) at (2,1.5) {};
\node [draw=black, align=center, circle] (h03) at (2,0.5) {};

\node [draw=black, align=center, circle] (h1) at (4,2.5) {}; 
\node [draw=black, align=center, circle] (h2) at (4,1.5) {};
\node [draw=black, align=center, circle] (h3) at (4,0.5) {};

\node [draw=black, align=center, circle] (o1) at (6,2) {};
\node [draw=black, align=center, circle] (o2) at (6,1) {};

\draw[arrow]	(i1) -- (h01);
\draw[arrow]	(i1) -- (h02);
\draw[arrow]	(i1) -- (h03);

\draw[arrow]	(i2) -- (h01);
\draw[arrow]	(i2) -- (h02);
\draw[arrow]	(i2) -- (h03);

\draw[arrow]	(i3) -- (h01);
\draw[arrow]	(i3) -- (h02);
\draw[arrow]	(i3) -- (h03);

\draw[arrow]	(i4) -- (h01);
\draw[arrow]	(i4) -- (h02);
\draw[arrow]	(i4) -- (h03);

\draw[arrow]	(h1) -- (o1);
\draw[arrow]	(h1) -- (o2);

\draw[arrow]	(h2) -- (o1);
\draw[arrow]	(h2) -- (o2);

\draw[arrow]	(h3) -- (o1);
\draw[arrow]	(h3) -- (o2);

\node[draw=black,fit=(i1) (i2) (i3) (i4), ellipse] (inputLayer) {};
\node[draw=black,fit=(h01) (h02) (h03), ellipse] (hiddenLayer) {};
\node[draw=black,fit=(h1) (h2) (h3), ellipse] (hiddenLayer) {};
\node[draw=black,fit=(o1) (o2), ellipse] (outputLayer) {};

\node [align=center] (x) at (-1,1.5) {$\overrightarrow{x}$};

\node [align=center] (di) at (0,5) {\textcolor{black}Input\\ layer\\ $L_1$};
\node [align=center] (dh1) at (2,5) {\textcolor{black}Hidden\\ layer\\ $L_{2}$};
\node [align=center] (dh1) at (3,1.5) {$\cdots$};
\node [align=center] (dh2) at (4,5) {\textcolor{black}Hidden\\ layer\\ $L_{n-1}$};
\node [align=center] (do) at (6,5) {\textcolor{black}Output\\ layer\\ $L_n$};

\node [align=center] (y1) at (7,1.5) {$\overrightarrow{y}$};

\end{tikzpicture}
\caption{An example of an artificial neural network with $n$ layers. Its network topology is $(|L_1|=4,|L_2|=3,...,|L_{n-1}|=3,|L_n|=2)$}
\label{fig:layer}
\end{figure}

Particular significance is drawn to the layers in between input and output layer. Those layers are optional, but needed to approximate certain 
functions such as the XOR-Gate.\cite{XOR} As they are invisible to an application using the network as a black box, all neurons $n_i \notin L_1 \wedge n_i \notin L_n$
are called hidden neurons and are organised in hidden layers. The set of hidden layers is denoted as $H$: 
\begin{equation}
H \subseteq L; H = \{L_i|1<i<n\}
\end{equation} 

A network where $H \neq \oldemptyset$ is called a multi-layer network (MLN). 

\subsection{Units}
\label{subsec:units}

A first improvement to McCulloch and Pitt's model was the concept of weighted connection links. Inspired by the function of the synaptic 
terminals, a weight $w_{ij}$ between two neurons $n_i$ and $n_j$ models the magnitude of influence that a signal from $n_i$ has on the state of $n_j$.
Weights may be both positive and negative, exhibatory or inhibatory. A magnitude of zero stands for a cut-off signal flow. 
On the implementation side, the entirety of connection weights within a network is often stored in a matrix, which allows for cheap manipulation of the weights. 
The manipulation of connection weights may be used to alter the network's behaviour. Altering the weights in a way that allows to more accurately approximates
a function is referred to as \textbf{learning} of a neural network. We will show means by which learning can be achieved in chapter \ref{ch:learning}.

$W$ is called the connection matrix, which fully represents the structure of a feedforward network. A simple example where $H = \emptyset$ is given below. 
\begin{equation}
W = 
\begin{pmatrix}
w_{11} & \cdots & w_{i1} \\
\vdots & \ddots & \vdots \\
w_{1j} & \cdots & w_{ij} \\
\end{pmatrix}
\end{equation}

In addition, the concept of bias neurons has proven itself useful. Bias units are non-receptive, both in feedforward and recurrent networks. 
Hence their activity level is steady, usually $+1$. Links between bias and regular neurons do not differ from normal links. Hence, they are 
associated with a weight $w_{ij}$ which may be altered. Bias neurons are often used to keep units with little weighted input active, 
which is done by shifting their activation function $\varphi$. This can be necessary if a neuron does not receive significant input. 
In contrast, a negative weight may be used to keep a neuron in an inactive state.
%TODO: Activation must be mentioned earlier
\begin{figure}[H]

\centering
\begin{subfigure}{.5\textwidth}
  \centering
    \begin{tikzpicture}
        \node [draw=black, align=center, circle] (i) at (0,0) {$n_0$}; 
        \node [draw=black, align=center, circle] (o) at (4,0) {$n_1$}; 
        \draw[arrow]	(i) -- (o);

        \node[] (dw) at (2,0.3){$w_{ij}$};
        \node[] (di) at (0,0.6){Input: $x$};
        \node[] (do) at (4,0.6){Output};
        \node[] (df) at (2.1,-0.7){$\varphi = sig(w_{01}\cdot x)$};
    \end{tikzpicture}
  \caption{A single input-output network.}
  \label{fig:1on1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
    \centering
        \begin{tikzpicture}
        \node [draw=black, align=center, circle] (i2) at (0,2) {$n_0$}; 
        \node [draw=black, align=center, circle] (i1) at (0,0) {$n_1$}; 
        \node [draw=black, align=center, circle] (o) at (4,1)  {$n_2$}; 

        \draw[arrow]	(i2) -- (o);
        \draw[arrow]	(i1) -- (o);

        \node[] (dw) at (2,1.8){$w_{02}$};
        \node[] (dw) at (2,0.8){$w_{12}$};
        \node[] (di) at (0,0.6){Bias: $1.0$};
        \node[] (di) at (0,2.6){Input: $x$};
        \node[] (do) at (4,1.6){Output};
        \node[] (df) at (2.1,-0.7){$\varphi = sig(w_{02}\cdot x + w_{12}\cdot 1.0)$};
        \end{tikzpicture}
    \caption{A single input-output network with an additional bias neuron.}
    \label{fig:2on1}
\end{subfigure}
\newline
\caption{Scaling and shifting the input function}
\label{fig:networks}
\end{figure}

Consider the network shown in figure \ref{fig:1on1}. The activation function of the output neuron is chosen to be the sigmoid (abbreviated: sig) 
function. Figure \ref{fig:sigmoid-scale} shows the output for several values of $w$. It can seen that $w$ changes the steepness of the 
sigmoid function. 

Figure \ref{fig:2on1} shows the same network with an additional bias neuron. This allows us to shift the activation function according to the 
weight on the connection link $w_{12}$ as shown in figure \ref{fig:sigmoid-shift}.

\begin{figure}[H]

\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1.1\linewidth]{files/activation/sigsc.pdf}
  \caption{The output of unit $n_1$ in figure \ref{fig:1on1}}
  \label{fig:sigmoid-scale}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1.1\linewidth]{files/activation/sigsh.pdf}
  \caption{The output of unit $n_1$ in figure \ref{fig:2on1}}
  \label{fig:sigmoid-shift}
\end{subfigure}
\newline
\caption{Scaling and shifting the input function}
\label{fig:scaling}
\end{figure}

\subsection{Activation functions}

As mentioned before, the weights of the connections between the neurons model the behaviour of the network. The bigger the absolute 
value of the weight, the greater the influence of a neuron to another. This value can be both positive and negative, as well as 
neutral \textit{(0)}, meaning that there is currently no influence between the neurons, although there is still a link connecting them. 
A weight of 0 can be understood as a logical pinch of the link, preventing it from transmitting signals.


The connection between the input values and the activity level of the neuron is made by the activity function $\varphi$. Different models are 
obtained depending on the choice of the activity or transfer function. Among the most common choices are the binary step (as used by McCulloch and Pitts), 
sigmoid and hyperbolic tangent $\tanh()$ functions as illustrated in Figure \ref{fig:activations}. The step function represents the firing of a pulse down the axon 
if positive $1$), while $0$ represents no firing. The step function is sometimes defined to jump from $-1$ to $1$. 

Sigmoid functions are widely used among networks which are to represent cognitive processes such as perception, recognition, problem solving, 
etc. . 
The main advantage of a sigmoid functions are:

\begin{itemize}
\item The activity level of the function is limited in both the positive and the negative. This means that activity in the network can not spill 
      over unintentionally, which can be caused by recurrent connections. Note that this property may not be met by other functions (e.g. a linear one).
\item \textbf{Differentiability}
As distinct from the binary step function, it is differentiable at all parts, which as an example, is a requirement of the gradient descent,
used by the Backpropagation algorithm (about to be introduced in chapter \ref{ch:learning}.
\end{itemize}

\begin{figure}[H]

\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1.1\linewidth]{files/activation/step.pdf}
  \caption{The binary step function}
  \label{fig:plot1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=1.1\linewidth]{files/activation/sigmoid.pdf}
  \caption{A sigmoid ($f(x) = \frac{1}{1+e^{-x}}$) function}
  \label{fig:plot2}
\end{subfigure}
\newline
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{files/activation/tanh.pdf}
  \caption{The $tanh()$ function}
  \label{fig:plot3}
\end{subfigure}
\caption{Comparison of commonly used activation functions}
\label{fig:activations}
\end{figure}

During the work with neural networks, we distinguish between work and training phases. During training, the network learns certain behavior by 
feeding repetitive input to the first layer and observing the output of the network. This can be done in a supervised and unsupervised way. In a 
supervised approach, the targeted output of the network is already known and is presented to the network as a \textit{teaching factor}. The 
network produces output depending on the current weights on its connections and adjusts them according to the margin between the targeted and 
actual network output. Unsupervised learning is necessary in situations, where no reasonable training can be created or a utterly unknown.
Similarity between input incentives and connection weights is the factor, according to which weight adjustment is done in such models.

\section{Applications}
\label{sec:exp}
Until today, neural networks have been successfully applied to a wide range of problems in a variety of scientific and industrial applications. 
Although of greatly different nature, almost all of those applications can be reduced to either pattern/shape recognition or logical filter tasks. 
Examples are:\\
\begin{itemize}
    \item Online recognition of handwritten text.
    \item Autopilots in aerospace engineering.
    \item Noise suppression and signal filtering in communication technology.
    \item Object recognition.
    \item Weather forecast.
    \item Function approximation including time series prediction. 
\end{itemize} 

The following sections will give a brief notion of some ANN applications where their application has been noticeably more beneficial than approaches 
with other, non-ML techniques.

\subsection{Recognition of hand-writing}
The automated interpretation of hand-written text is of great interest for a number of purposes. The destination address of letters and parcels,
for instance, are nowadays processed by computers which use real-time image processing system to recognize handwriting. The typesetting system 
\LaTeX allows its users to formulate complex formulas using certain commands which are later being replaced by symbols such as the existential 
quantifier $\exists$. In case of an unknown command for a certain symbol, one would wish for a system that can take in a hand written representation 
of the symbol and output the command by which the symbol can later be written in a document. In \cite{MARTIN} artificial neural networks are used 
to classify such symbols. After drawing the sought symbol, the user is presented a choice of matching symbols along with an 
associated probability measure of their correctness. The application is available under \url{write-math.com}.
\\
%TODO: Really for that purpose, explain perceptron
The challenge for such systems is the great deviation of styles in hand-written text. This makes it virtually infeasible for conventional methods 
to reliably classify hand-writing. The network model RCE(Restricted Coulomb Energy Network) has been devised by Nobel price winner Leon Cooper for 
that very purpose. It overcomes the limitations of the perceptron as it is capable of detecting geometrical figures (therefore also handwritten text) 
of any size. Cooper's company Nestor has successfully used this model to develop a system which can (among others) detect characters of the Japanese alphabet \textit{Kanji}.
The Kanji alphabet stems from the Chinese language, contains approximately 3000 different symbols and is extremely complex. This is due to the great
variety of symbols which can be formed with lines arranged in a certain way. Examples are \begin{CJK}{UTF8}{min}輸\end{CJK} - transport or 
\begin{CJK}{UTF8}{min}熊\end{CJK} - bear. It is nowadays extensively used on mobile devises which allow its users to write text messages by drawing 
each individual character instead of choosing symbols from a massive dictionary. Complex sentences formulated by simultaneously using three different
alphabets, such as \begin{CJK}{UTF8}{min}世界はコンピュータ技術の登場以来、劇的に変化した。\end{CJK} - \textit{The world has changed dramatically 
since the advent of computer technology} can now be written in fairly short time.

\subsection{The Travelling-Salesman-Problem}
Given a set of cities and the distances between each pair of cities, what is the shortest path that, after visiting each city exactly once, 
leads back to the origin of the route? This task, called the Travelling-Salesman-Problem (often abbreviated as TSP) is an optimisation task 
often encountered in practice (Energy and water supply, microchip design, ...). The TSP can be modelled as a graph: All $n$ cities are represented 
as vertices, the connections between the cities as edges. Since all cities are connected with each other, the graph is complete. 
The cost (travel time) between each pair of vertices $i$ and $j$ is associated with a weight $w(i,j) \geq 0$. 
A tour that that leads to each city exactly once and returns back to the origin is a Hamilton-circle within the graph. As the graph is complete, 
there can great numbers Hamilton-circles (each of which is a possible solution of the problem). 
Each permutation of the nodes is a Hamilton-circle, thus there are $n!$ circles. Sought is the Hamilton-circle with the least overall cost, i.e. a 
permutation $\pi(1),\dots,\pi(n)$ with minimal weight
\begin{equation}
    \sum\limits_{i=1}^n w(\pi(i),\pi(i+1)), 
\end{equation}

where $\pi(n+1)$ is set to $\pi(1)$.\cite{MATHINF}\\

\begin{figure}[H]
\centering
    \begin{tikzpicture}
    \node [shape=circle, fill=black, inner sep=2pt] (x1) at (0.0,0.0) {}; 
    \node [shape=circle, fill=black, inner sep=2pt] (x2) at (0.0,2.0) {}; 
    \node [shape=circle, fill=black, inner sep=2pt] (x3) at (2.0,0.0) {}; 
    \node [shape=circle, fill=black, inner sep=2pt] (x4) at (2.0,2.0) {}; 
    \node [] (l1) at (-0.3,0.0) {1}; 
    \node [] (l2) at (-0.3,2.0) {2}; 
    \node [] (l3) at (2.3,0.0) {3}; 
    \node [] (l4) at (2.3,2.0) {4}; 
    \node [] (l5) at (2.3,1.0) {2}; 
    \node [] (l6) at (-0.3,1.0) {5}; 
    \node [] (l7) at (1,2.3) {4}; 
    \node [] (l8) at (1,-0.3) {8}; 
    \node [] (l9) at (0.75,1.55) {1}; 
    \node [] (l10) at (1.25,0.45) {3}; 

    \draw[] (x1) -- (x2);
    \draw[] (x1) -- (x4);
    \draw[] (x2) -- (x3);
    \draw[] (x2) -- (x4);
    \draw[] (x4) -- (x3);
    \draw[] (x3) -- (x1);
    \end{tikzpicture}
\caption{An example graph. We are are interested in finding the cheapest tour.\cite{MATHINF}}
\label{fig:tsp}
\end{figure}

An example graph for the TSP is given in \ref{fig:tsp}. $4!$ possible tours exit, which is the amount of permutations of the vertices.
We find the cheapest tour $(1, 3, 4, 2, 1)$ with costs of $w(1, 3) + w(3, 4) + w(4, 2) + w(2, 1) = 11$.

The TSP is of high importance in theoretical computer science, as it is known to be $NP$-complete. In the field of complexity analysis, 
the class of decision problems for which there is a deterministic Turing-Machine able to solve it in time $\mathcal{O}(n^k)$ is called $P$ 
(polynomial). An example is the sorting problem. Class $P$ contains problems with running times like $\mathcal{O}(n)$ and $\mathcal{O}(log(n))$ but 
also those with time $\mathcal{O}(n^{5000})$. In addition to the deterministic Turing machine, complexity analysis also defines the non-deterministic
Turing machine. At each time step, there are several possible ways of continuing computation, setting it in contrast with todays computers. The 
class of Problems $NP$ is solvable by a non-deterministic Turing Machine in polynomial time. $NP$ contains a number of relevant problems, such as 
the graph isomorphism (can two graphs be drawn identically?). Problems in $P$ can also be solved by this theoretical model, so $P \subseteq NP$. 
In other words, a problem is in the class of $NP$ if there is an algorithm (executed by a Turing machine) that can guess a solution and verify 
whether the guess was correct in polynomial time. In case of omniscient guessing or given a number of processors equal to the number of solutions, 
trying out all possibilities at the same time one would always get the correct solution. Thus, $NP$ problems would become $P$ problems. 


Nevertheless, computationally feasible approximation techniques have been found which allow to find a good, although not always the best, solution. 
Hopfield and Tank (1985) proposed an approach using ANNs which allows to find a good, often even the best solution for the TSP. It suggests a 
single-layer network with $n$ neurons, where $n$ equals the number of cities in the graph, called Hopfield-network. This can be thought of as a 
square with an edge length of $n$. The output of each individual neuron equals the time step at which the city is visited. Furthermore, 
genetic algorithms have also been applied to find an approximate solution for the problem.\cite{GENTSP}

\chapter{Learning}
\label{ch:learning}
\section{Hebbian learning rule}
The psychologist Donald Olding Hebb proposed one of the easiest learning rules with high biological plausibility in \cite{HEBB}:\\
\textit{“Let us assume that the persistence or repetition of a reverberatory activity (or ‘trace’) tends to induce lasting cellular changes that 
add to its stability\dots When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some 
growth process or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased.”}\\

In summary: The weight between two units $i$ and $j$ is changed, if both units are active at the same time. The amount of change is set by three 
factors:

\begin{itemize}
\item The activity level of the sending unit $a_i$.
\item The activity level of the receiving unit $a_j$.
\item A settled and constant parameter $\eta$.
\end{itemize}

The weight change $\Delta w_{ij}$ using the bias learning rule is shown in equation \eqref{eq:hebb}.
\begin{equation}
\Delta w_{ij} = \eta \cdot a_i \cdot a_j
\label{eq:hebb}
\end{equation}

One of the major drawbacks of the Hebbian rule is the constraint, that is can only applicable to networks without a hidden layer (non-MLN), 
and therefore only very simple neural networks.

\section{Delta rule}
The delta rule is based on the comparison between a targeted output $t$ and the observed output $o$ and thus constitutes an example of 
supervised learning. It can be expressed using with following equation:
\begin{equation}
\delta = t - o
\end{equation}

The following possibilities can be observed:
\begin{itemize}
\item \textbf{The observed output is too low}\\
In order to amplify the output, the weights between neurons is strengthened, provided a positive weight on the connection linking them. 
Negative weights are weakened.
\item \textbf{The observed output is too high}\\
All connections with positive weights are weakened, while the ones with negative weights are strengthened.
\item \textbf{Targeted and observed output are equivalent. (= ideal behavior)}\\ There is no need of link amendment.
\end{itemize}

The delta learning rule equation shown in \eqref{eq:delta2} covers all three possibilities, thus ensuring that the changing rate is proportional to 
the difference between targeted and observed output. The learning parameter $\eta$ is again set before the beginning of the learning process and 
remains constant. The multiplication with the sending units output $a_i$ ensures that those connections, that have the greatest influence on the 
error also have the greatest $\delta w_{ij}$.
\begin{equation}
\Delta w_{ij} = \eta \cdot \delta \cdot a_i
\label{eq:delta2}
\end{equation}

The simplicity of this learning rules also comes at great cost: The comparison only allows us to adjust connections which connect to the rightmost 
layer in the network. This fact restricts the application of this method to MLN networks.

\section{Backpropagation}

\subsection{Gradient descent}

The optimisation algorithm gradient descent (also steepest descent) tackles the problem of finding a local minimum of a multivariate function $F(x)$.
In order to do, one subtracts the gradient $\nabla F(a)$ from the current point $a$ to obtain a new point $b$ so that $F(a) \geq F(b)$. 
The non-negative weight $\gamma$ is often referred to as the step size.

\begin{equation}
 b = a - \gamma \nabla F(a)
\label{eq:graddes}
\end{equation} 

The gradient of a differentiable function $f: D \subseteq \mathds{R}^n$ $\rightarrow$ $\mathds{R}$ must disappear at a local minimum or maximum 
at $x_i$ (\cite{MATHINF}):

\begin{equation}
\nabla F(x_i) = 0
\end{equation} 

Starting with guess $x_0$ for a local minimum, and iteratively calculating an $x_{i+1}$ according to equation \eqref{eq:graddes}, we will obtain 
$x_0, ..., x_i$ so that $F(x_0) \geq F(x_1) \geq ... \geq F(x_{n-1}) \geq F(x_n)$. The parameters $\theta$ and $n$ are used as an ending 
criteria for the algorithm. This is shown in algorithm \ref{alg:GD}.

\begin{algorithm}
\LinesNumbered
\DontPrintSemicolon
\BlankLine
\Ein{A differentiable function $F(x)$, step size $\gamma$, tolerance $\theta$, point $x_0$}
\Aus{A local minimum at $x_i$}
\BlankLine
\Begin
{
    \Repeat{$\Delta x < \theta$ for $n$ iterations}
    {
        $x_{i+1}$ = $x_i - \gamma \nabla F(x_i)$
    }
}
\caption{The gradient descent algorithm.}
\label{alg:GD}
\end{algorithm}

Gradient descent may be imagined as the effort of a hiker to reach the bottom of valley as quick as possible, therefore always seeking 
to climb down as the point of steepest descent. This is illustrated in figure \ref{fig:grad}.
In other words, gradient descent starts with a given combination of weights (often random) for which the gradient is determined. The gradient is 
the function of a scalar field which shows the rate of change and the direction of the greatest change in the form of a vector field. After the 
gradient is determined it will be stepped down at a given rate - the learning rate, meaning that the weights are adjusted. For this new combination 
of weights, the gradient is again determined and stepped down until a local (or global minimum) is found or a maximum amount of repetitions is 
reached.

\begin{figure}[H]

    \centering
    \includegraphics[width=0.3\textwidth,scale=1]{files/graddes.png} 
    \caption{Illustration of gradient descent.\cite{GRADFIG}}
    \label{fig:grad}
\end{figure}

\subsection{The Backpropagation algorithm}

The Backpropagation algorithm is yet another learning rule for supervised networks, but in comparison to the Hebbian and delta rules, it can also 
be applied to networks with $n$ numbers of hidden layers. There are certain problems that cannot be solved by networks without at least one 
hidden layer, the XOR-Gate being a famous example. It was developed by several scientists simultaneously \cite{BACK1}\cite{BACK2}\cite{BACK3} and 
consists of three stages:
\begin{itemize}
\item[1.] \textbf{The forward-pass}:
A pattern is presented to the input layer and further propagated into the network.
\item[2.] \textbf{Error-determination}:\\
The observed output is compared to the targeted result and an error $E$ is determined by a derivable transfer function $\varphi$ shown in equation 
\eqref{eq:backerror}. The factor of $\frac{1}{2}$ is used to simplify the derivation by cancelling the exponent.

\begin{equation}
\varphi = \frac{1}{2} (t-o)^2
\label{eq:backerror}
\end{equation}
\item[3.] \textbf{Backward-pass}:\\
The error is propagated from the back of the network to the front, hence the name \textbf{Backpropagation}. The connections are updated independently 
from their influence to $E$, which guarantees a result $o$ that is closer to $t$, provided the same output is presented to the input layer.
\end{itemize}

A first approach on finding a method to changing all weights in a network, which minimizes the error, is to find $E$ for all possible combinations 
of weights $W$. The combination $W_{min}$ with the smallest $E$ would be the perfect solution, an absolute minimum of the error. The problem of this 
approach is that the computational effort to finding this solution is way to high, since $W_{min}$ would have to be found in a $n$-dimensional 
hyperplane (Where $n$ is equal to the number of neurons).\\

Instead, the weights are adjusted using gradient descent, for which explores the hyperplane does not need to be known in advance. 
The amount of change between neurons $i$ and $j$ $\delta w_{ij}$ is then calculated according to \eqref{eq:backrule}.
\begin{equation}
\Delta w_{ij} = -\eta \frac{\partial E}{\partial w_{ij}} = \eta \cdot \delta_j \cdot a_i
\label{eq:backrule}
\end{equation}
As distinct form the delta rule, the Backpropagation rule differs two cases, shown in equation \eqref{eq:backcases}. 
$k$ is the index of the neurons in the subsequent layer.
\begin{equation}
   \delta_j =
   \begin{cases}
     \varphi'(in_j)(t_j-o_j) & \text{In case $j$ is an output neuron} \\
     \varphi'(in_j)\sum_{k} \delta_k \cdot w_{j,k} & \text{In case $j$ is a hidden neuron}
   \end{cases}
\label{eq:backcases}
\end{equation}

Eventually, the weights are changed according to equation \eqref{eq:backweight}.

\begin{equation}
   w_{ij}^{new} = w_{ij}^{old} + \Delta w_{ij}
\label{eq:backweight}
\end{equation}

\section{Backpropagation through time}

The Backpropagation through time (BTT) algorithm is a learning rule for recurrent networks (Also referred to as RNNs). 
RNNs constitute an architecture that allows bi-directional signal flow within the network. While links of feedforward networks are constraint to 
lead only to neurons in the subsequent layer, connections can now be formed across arbitrary layers. Figure \ref{fig:feedback} shows a 
simple recurrent network (Also called Elman networks). As each neuron is connected with itself, it forms a so-called feedback structure. 
The output of the neuron at time $t$ serves as a weighted (by weight $g$) input of itself in the next time step.
Thus, the output of the network can be computed as follows:

\begin{equation}
    y(t+1) = \varphi(w\cdot x(t) + g\cdot y(t))
\label{eq:btt}
\end{equation}

BTT was independently developed by several researchers shortly after the proposition of standard Backpropagation.\cite{BTT1}\cite{BTT2}\cite{BTT3}
As Backpropagation in its pure form is unable to cope with bi-directional signal flow, the rationale behind BTT is a dynamic structure which alters
between application and training. Figure \ref{fig:unrolled} shows this architectural transformation called “unrolling over time”. Given a certain 
depth, the feedback structure is transformed into a very deep feedforward structure. As the original inputs of each time step are preserved,
training can now take place according to algorithm \ref{alg:BTT}. The interested reader may refer to \cite{DEEPLEARNING} to further information on
Backpropagation through time.

\begin{figure}[H]

\centering
    \begin{tikzpicture}
    \node [align=center] (l1) at (0,0) {}; 
    \node [align=center] (x2) at (0.0,0.3) {$x(t)$}; 
    \node [align=center] (x1) at (0.75,0.3) {$w$}; 
    \node [draw=black, align=center, circle] (n) at (1.5,0) {}; 
    \node [align=center] (x2) at (2.95,0.3) {$y(t+1)$}; 
    \node [align=center] (x3) at (1.6,-0.7) {$g$}; 
    \node [align=center] (l2) at (3,0) {}; 

    \draw[arrow]	(l1) -- (n);
    \draw[arrow]	(n) -- (l2);
    \draw[arrow] (n) to [out=330,in=250,looseness=8] (n);
    \end{tikzpicture}
\caption{A feedback structure in a recurrent neural network.}
\label{fig:feedback}
\end{figure}

\begin{figure}[H]

\centering
    \begin{tikzpicture}
    \node [align=center] (n0) at (0,0) {}; 
    \node [] (y0) at (0.75,0.3) {$y(0)$}; 

    \node [] (g1) at (0.75,-0.2) {$g$}; 
    \node [] (w1) at (1.50,0.7) {$w$}; 
    \node [] (x1) at (1.0,1.3) {$x(0)$}; 
    \node [] (t1) at (1.5,-0.7) {$t-1$}; 
    \node [] (y1) at (2.25,0.3) {$y(1)$}; 
    \node [draw=black, align=center, circle] (n1) at (1.5,0) {}; 

    \node [] (g2) at (2.25,-0.2) {$g$}; 
    \node [] (w2) at (3.00,0.7) {$w$}; 
    \node [] (x2) at (2.5,1.3) {$x(1)$}; 
    \node [] (t2) at (3,-0.7) {$t-2$}; 
    \node [] (y2) at (3.75,0.3) {$y(2)$}; 
    \node [draw=black, align=center, circle] (n2) at (3,0) {}; 

    \node [] (g3) at (3.75,-0.2) {$g$}; 
    \node [] (w3) at (4.50,0.7) {$w$}; 
    \node [] (x3) at (4.0,1.3) {$x(2)$}; 
    \node [] (t3) at (4.5,-0.7) {$t-3$}; 
    \node [] (y3) at (5.25,0.3) {$y(3)$}; 
    \node [draw=black, align=center, circle] (n3) at (4.5,0) {}; 

    \node [] (g4) at (5.25,-0.2) {$g$}; 
    \node [] (w4) at (6.00,0.7) {$w$}; 
    \node [] (x4) at (5.5,1.3) {$x(3)$}; 
    \node [] (t4) at (6.0,-0.7) {$t-4$}; 
    \node [] (y4) at (6.75,0.3) {$y(4)$}; 
    \node [draw=black, align=center, circle] (n4) at (6,0) {}; 

    \node [] (g5) at (6.75,-0.2) {$g$}; 
    \node [] (w5) at (7.50,0.7) {$w$}; 
    \node [] (x5) at (7.0,1.3) {$x(4)$}; 
    \node [] (t5) at (7.5,-0.7) {$t-5$}; 
    \node [] (y5) at (8.25,0.3) {$y(5)$}; 
    \node [draw=black, align=center, circle] (n5) at (7.5,0) {}; 

    \node [align=center] (n6) at (9,0) {}; 

    \draw[arrow]	(n0) -- (n1);
    \draw[arrow]	(x1) -- (n1);
    \draw[arrow]	(n1) -- (n2);
    \draw[arrow]	(x2) -- (n2);
    \draw[arrow]	(n2) -- (n3);
    \draw[arrow]	(x3) -- (n3);
    \draw[arrow]	(n3) -- (n4);
    \draw[arrow]	(x4) -- (n4);
    \draw[arrow]	(n4) -- (n5);
    \draw[arrow]	(x5) -- (n5);
    \draw[arrow]	(n5) -- (n6);
    \end{tikzpicture}
    \caption{The recurrent network shown in figure \ref{fig:feedback} during unfolding with depth $t=5$.}
\label{fig:unrolled}
\end{figure}

\begin{algorithm}[H]
\LinesNumbered
\DontPrintSemicolon
\BlankLine
\Ein{Input $x(t)$ at time $t$, depth $k$}
\Aus{Output $y(t)$}
\BlankLine
\Begin
{
    Unfold network to contain $k$ instances.\\
    $y(0) \leftarrow \text{zero-magnitude vector} \overrightarrow{0}$.
    
    \For{$\forall t \in [0,n-1]: t \in \mathbb{N}_0$}
    {
        \tcp{Done for each }
        Set network inputs $y(0), x(1), x(2), x(3)$\\
        Forward-propagate.\\ 
        Error $e = y(t+k) - t;$ $t:$ target error\\
        Backpropagate the error across the whole unfolded network.\\
        Update all the weights in the network according to standard Backpropagation.\\
        $y(0) \leftarrow f(x)$

    
    }
}
\caption{The Backpropagation through time algorithm.}
\label{alg:BTT}
\end{algorithm}

\section{Genetic algorithms}
\textit{Since this project builds off of Schembri's work, we assume knowledge about the hill-climbing approach in unsupervised learning described in 
\cite{DANIEL}. Readers not familiar with hill-climbing and artificial evolution are advised to refer to the original paper before further reading.} 

\subsection{Crossover}
The unsupervised training of neural networks is greatly inspired by evolution: The notion of natural selection was first presented by Charles Darwin 
in his book “\textit{On the origin of species by means of natural selection}”\cite{DARWIN}. As a comprehensive discussion of Darwin's ideas is 
outside the scope of this paper, we will from here on refer to evolution as the following definitions \cite{OXFORD}:

\begin{center}
\textit{“Change in the genetic composition of a population during successive generations, often resulting in the development of new species. 
The mechanisms of evolution include natural selection acting on the genetic variation among individuals, mutation, migration, and genetic drift.”}\\
\end{center}

and

\begin{center}
\textit{“A gradual process in which something changes into a different and usually more complex or \textbf{better form}.”}\\ 
\end{center}

In particular, we are especially interested in a change or modification which results in a new entity (neural network) better accustomed to the 
respective demands of the environment. We rate an ANN as better accustomed when its performance measure (i.e. the fitness function) has become 
higher. The hill-climbing algorithm tries to achieve this by modifications which continuously alter the weights of the respective network so 
as to maximise performance. However, this method may not reach a global maximum in case of local minima or flat parts of the respective fitness 
function.

Crossover spins the idea of artificial evolution further, still. Instead of merely keeping the best individual of each population and modifying 
its weights slightly in hope for better results, crossover implements the idea of artificial reproduction. Reproduction is implemented by a 
combination of the weights of two chosen networks according to specific criteria. The Crossover algorithm is usually used alongside hill-climbing 
to overcome the pitfall of local minima. We will refer to this combination of as \textit{augmented genetic algorithm}.

Figure \ref{fig:ag} shows the principle structure of the augmented genetic algorithm. A thorough explanation is also given in \cite{CROSSOVER}.
After a population has been applied in an environment, the fitness of each individual is ranked according to their performance. 
In addition, a selection process choses certain networks according to a selection function. Individuals are either selected for reproduction 
or direct mutation. Direct mutation is often used to clone the best-performing networks of the current population (sometimes called elitism). 
In case of direct mutation, the augmented genetic algorithm does not differ from the standard hill-climbing approach.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
    \node [draw=black, fill=white, align=center] (g) at (2,0) {Generation of initial individuals};
    \node [draw=black, fill=white, align=center] (e) at (2,-1) {Application};
    \node [draw=black, fill=white, align=center] (r) at (2,-2) {Fitness ranking};
    \node [draw=black, fill=white, align=center] (s) at (2,-3) {Selection};
    \node [draw=black, fill=white, align=center] (c) at (0,-4) {Crossover};
    \node [draw=black, fill=white, align=center] (m) at (4,-4) {Mutation};

    \draw[arrow]    (g) -- (e);
    \draw[arrow]    (e) -- (r);
    \draw[arrow]    (r) -- (s);
    \draw[arrow]    (s) -- (c);
    \draw[arrow]    (s) -- (m);
    \draw[arrow]    (c) -- (m);
    \draw[arrow]    (c) to[out=-180,in=-180] (e);
    \draw[arrow]    (m) to[out=0,in=0] (e);
    
    \end{tikzpicture}
\caption{The principle structure of the augmented genetic algorithm.\cite{CROSSOVER}}
\label{fig:ag}
\end{figure}

\paragraph{Selection}
In selection, one might be inclined to presume that only well-performing agents ought to be taken into consideration for mating. However, creating
offspring from both a well and a poorly performing agents can help to increase their fitness \cite{KIKLAR}. This is why none
of the networks must be excluded from the selection process. However, a bias towards better fitness is still desirable. This is implemented by 
increasing the chances of picking better agents. One might think of the selection process as a game of roulette, where the sizes of the
fields vary. Each field stands for a network of the previous population, their size corresponds to their fitness. As in standard-roulette, each
field may be chosen, some with a higher probability than others. An example of the selection process is given in figure \ref{fig:roul}. After the 
application of a five-agent population, each one is ranked (as shown in table \ref{tab:rank} according to the amount of collected objects. 
Subsequently, a random number $\in (0,1)$ is chosen. As each slice itself is an interval, the slice containing the random number marks an entity 
chosen for reproduction. The genes of the corresponding agent are then modified in reproduction to create offspring.
  
\begin{minipage}{\textwidth}
  \begin{minipage}[b]{0.49\textwidth}
    \centering
        \begin{tikzpicture}[scale=0.5]
            \pie[color={black!10, black!15, black!20, black!25, black!30}, after number=]
            {5/E , 15/D , 15/C , 25/B, 40/A}
        \end{tikzpicture}
        \captionof{figure}{The probability distribution during the selection process. The size of each slice corresponds
                           to the score of the corresponding agent.}
        \label{fig:roul}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \centering
      \begin{tabular}{|c|c|} \hline
         Agent & Score \\ \hline
         A & 40 \\ \hline
         B & 25 \\ \hline
         C & 15 \\ \hline
         D & 15 \\ \hline
         E & 5  \\ \hline
      \end{tabular}
          \captionof{table}{An example fitness ranking of a five agent population. The agents are ranked according to
                            their fitness, which corresponds to the amount of objects found.}
          \label{tab:rank}
    \end{minipage}
  \end{minipage}

\paragraph{Reproduction}
If two individuals are chosen for reproduction, they create a certain amount of children according to the rule of genetic combination. 
As an analogy to human reproduction, the selected networks will be referred to as the male and female part. Figure \ref{fig:repro} illustrates 
this process. 

\begin{figure}[H]
	\centering
	\begin{tikzpicture}
	\node [] (label) at (-2,0) {Parents};
	\node [draw=black, fill=white, align=center, circle] (m) at (0,0) {\mars};
	\node [draw=black, fill=white, align=center, circle] (f) at (3,0) {\female};

    \node [draw=black, fill=white, align=center] (co) at (1.5,-1) {Crossover};

	\node [] (label) at (-2,-2) {Children};
	\node [draw=black, fill=white, align=center, circle] (c1) at (0,-2) {1};
	\node [draw=white, fill=white, align=center] (c2) at (1.5,-2) {\dots};
	\node [draw=black, fill=white, align=center, circle] (c3) at (3,-2) {n};

	\draw[arrow]	(m) -- (co);
	\draw[arrow]	(f) -- (co);
	\draw[arrow]	(co) -- (c1);
	\draw[arrow]	(co) -- (c2);
	\draw[arrow]	(co) -- (c3);
	
	\end{tikzpicture}
	\caption{An illustration of the crossover reproduction model. Two parent networks are unified two create offspring.}
	\label{fig:repro}
\end{figure}

One-Point Crossover is an example of a combination rule for two children. During the mating process, a random integer, called crossover point ($CP$),
is chosen. It determines the ratio of male and female genes in the offspring. Given that number, the genetic combination can finally take place.
When we talk about genes in the context of ANNs, we are referring to the weights of the respective network. The crossover point simply states the number 
of connection weights cloned from each parent to form the offspring. The total number of weights in an ANN is equal to the number of elements in the 
weight matrix $W$ (see section \ref{subsec:units}) and can be calculated as follows: $|W| = \sum^{n-1}_{i=1} L_i \cdot L_{i+1}$. 
For the first of the two children, $CP$ is equal to the number of weights taken from the first parent, while $|W|-CP$ states the amount from the 
second parent. Conversely, $CP$ equals the weights taken from the second parent, while $|W|-CP$ states the amount from the first parent. 
Figure \ref{fig:matrixcross} shows an example of one-point crossover for an ANN of topology (3,3). The crossover point has been chosen to be $5$ in 
this example. Other crossover rules are two point crossover, uniform crossover, where genes are copied at random \cite{UNIFORMCROSSOVER} and selective 
crossover\cite{SELECTCROSSOVER}.

\begin{figure}[H]

    \begin{center}
        $
        W_{\mars} = 
        \begin{pmatrix}
        \textcolor{blue}{w_{11}} & \textcolor{blue}{w_{21}} & \textcolor{blue}{w_{31}} \\
        \textcolor{blue}{w_{12}} & \textcolor{blue}{w_{22}} & \textcolor{blue}{w_{32}} \\
        \textcolor{blue}{w_{13}} & \textcolor{blue}{w_{23}} & \textcolor{blue}{w_{33}} \\
        \end{pmatrix}
        W_{\female} = 
        \begin{pmatrix}
        \textcolor{red}{w_{11}} & \textcolor{red}{w_{21}} & \textcolor{red}{w_{31}} \\
        \textcolor{red}{w_{12}} & \textcolor{red}{w_{22}} & \textcolor{red}{w_{32}} \\
        \textcolor{red}{w_{13}} & \textcolor{red}{w_{23}} & \textcolor{red}{w_{33}} \\
        \end{pmatrix}
        $
        \end{center}

        \begin{center}
        $
        W_{\text{Child 1}} = 
        \begin{pmatrix}
        \textcolor{blue}{w_{11}} & \textcolor{blue}{w_{21}} & \textcolor{blue}{w_{31}} \\
        \textcolor{blue}{w_{12}} & \textcolor{blue}{w_{22}} & \textcolor{red}{w_{32}} \\
        \textcolor{red}{w_{13}} & \textcolor{red}{w_{23}} & \textcolor{red}{w_{33}} \\
        \end{pmatrix}
        W_{\text{Child 2}} = 
        \begin{pmatrix}
        \textcolor{red}{w_{11}} & \textcolor{red}{w_{21}} & \textcolor{red}{w_{31}} \\
        \textcolor{red}{w_{12}} & \textcolor{red}{w_{22}} & \textcolor{blue}{w_{32}} \\
        \textcolor{blue}{w_{13}} & \textcolor{blue}{w_{23}} & \textcolor{blue}{w_{33}} \\
        \end{pmatrix}
        $
    \end{center}

	\caption{A One-Point Crossover application on a neural network given by topology $(3,3)$. We combine the genes (weights) of the
neural networks to create two children. The blue weights indicate the genes of the male parent network, wheres the 
red weights originate from the mother network.}
	\label{fig:matrixcross}
\end{figure}

\chapter{Neural networks for agent control}
\label{ch:design}

\section{Control parameters and operation considerations}
In this chapter, we will show how neural networks can be used to control the agent described in chapter \ref{ch:intro}. These agents are 
characterized by their sensors which observe the environment in a limited range given by a simulation parameter. Agents are only capable 
of looking forward, but can observe objects straight to their left, so that their sight forms a semi-circle of radius $r$ (the sight range) above 
the agent's head. The sensors will only receive information when an object is in sight. In such a case, the logic (which we strive to achieve with 
a neural network) will process this information and output values that will move the object in the direction of the object. As we make use
of Schembri's simulator, we need to obey the steering commands by which an agent's motion is achieved. They are:

\begin{itemize}
    \item $v_a \in [0.2,1]$, a scalar velocity command executed in a given direction.
    \item $\beta_a \in [\frac{-\pi}{2} rad, \frac{\pi}{2} rad]$, the steering angle in radians.
\end{itemize}

Where $\beta_a = \frac{-\pi}{2}$ equals a steering to the right, $\beta_a = 0$ straight motion and likewise, $\beta_a = \frac{\pi}{2}$ stands for 
an amendment of the heading direction to the left. This is illustrated in figure \ref{fig:acontrol}. The velocity command $v_a$ is in 
theory $ \in [0,1]$ but will be confined to be $\geq 0.2$ so that an agent never stops. In such a case, the agent will only receive sensory 
information if an object is dropped in his sight, which takes a indeterminate time, due to the randomness of object placement.

\begin{center}
\begin{figure}[H]

\begin{tikzpicture}[scale=3.1,cap=round,>=latex,rotate=90]
    % draw the coordinates
    \draw[->] (0cm,1.5cm) -- (0cm,-1.5cm) node[right,fill=white] {$x$};
    \draw[->] (0cm,0cm) -- (1.5cm,0cm) node[above,fill=white] {$y$};
    
    % draw the unit circle
    \draw [black,thick,domain=0:90] plot ({cos(\x)}, {sin(\x)});
    \draw [black,thick,domain=-90:0] plot ({cos(\x)}, {sin(\x)});
    
    \foreach \x in {-90,-60,...,90} {
            % lines from center to point
            \draw[gray] (0cm,0cm) -- (\x:1cm);
            % dots at each point
            \filldraw[black] (\x:1cm) circle(0.4pt);
            % draw each angle in degrees
            \draw (\x:0.6cm) node[fill=white] {$\x^\circ$};
    }
    
    % draw each angle in radians
    \foreach \x/\xtext in {
        -90/\frac{-\pi}{2},
        -60/\frac{-\pi}{3},
        -30/\frac{-\pi}{6},
        0/\frac{0}{0},
        30/\frac{\pi}{6},
        60/\frac{\pi}{3},
        90/\frac{\pi}{2}
    }
    \draw (\x:0.85cm) node[fill=white] {$\xtext$};
    
    % draw the horizontal and vertical coordinates
    % the placement is better this way
    \draw (0cm,-1.25cm) node[above=1pt] {$(1,0)$}
          (1.25cm,0cm)  node[above=1pt] {$(0,1)$}
          (0cm,1.25cm)  node[above=1pt] {$(-1,0)$};
    
    % origin
    \draw[fill=black] (0,0) circle(.013);

\end{tikzpicture}
	\caption{A semicircle representing the possible range of angular steering commands for an agent.}
	\label{fig:acontrol}
\end{figure}
\end{center}

A successfully detected object within the neuron's sight is characterized by an object vector $\overrightarrow{o} \in \mathbb{R}$ pointing towards it. 
In case of several such objects we only aim to focus the closest one. Therefore, only those vector elements will be processed by the decision policy.
An example environment in which an agent observes two objects $\overrightarrow{o_1}$ and $\overrightarrow{o_2}$ is given in figure \ref{fig:2obj}. As
$||\overrightarrow{o_1}|| = \sqrt{(\frac{-3r}{5})^2 + (\frac{3r}{10})^2} > ||\overrightarrow{o_2}|| = \sqrt{(\frac{r}{5})^2 + (\frac{4r}{5})^2}$, the next steering commands only 
depend on $o_1$. The agent's heading direction must now be altered so as to point to the chosen obstacle $o_1$. This is done by setting the angle
parameter $\beta_a$ to the angle enclosed by the vector and the y-axis. Any velocity $v_a > 0$ will now lead an overlapping of the agent with the
object. If this happens, the object is collected and disappears from the world. In order to do this in a smooth fashion, we will set the velocity
command $v_a = ||o||, \text{ provided } 0.2 < ||o||$. The agent will therefore start moving into the direction of the obstacle with a gradually decreasing velocity.

\begin{center}
\begin{figure}[H]

\begin{tikzpicture}[scale=4.5,cap=round,>=latex]
    % draw the coordinates
    \draw[step=0.1,black!20,thin] (-1cm,0) grid (1cm,1cm);
    
    \node[] (origin) at (0,0) {}; 
    \draw[] (-0.6cm,0.3cm) node[blue] {};
    \draw[blue] (-0.6cm,0.37cm) node[] {$ o_1 = (\frac{-3r}{5}\ \frac{3r}{10})$};
    \draw[blue, ->] (origin) -- (-0.6cm,0.3cm);
    
    \draw[] (0.2cm,0.8cm) node[red] {};
    \draw[red] (0.22cm,0.86cm) node[] {$ o_2 = (\frac{r}{5}\ \frac{4r}{5})$};
    \draw[red, ->] (origin) -- (0.2cm, 0.8cm);
    
    \draw[->] (-1cm,0cm) -- (1cm,0cm) node[right,fill=white] {$x$};
    \draw[->] (0cm,0cm) -- (0cm,1cm) node[above,fill=white] {$y$};
    
    % draw the unit circle
    \draw [black,thick,domain=0:90] plot ({cos(\x)}, {sin(\x)});
    \draw [black,thick,domain=180:90] plot ({cos(\x)}, {sin(\x)});
    
    % draw the horizontal and vertical coordinates
    % the placement is better this way
    \draw (-1.15cm,0cm) node[above=1pt] {$(-r,0)$}
          (1.15cm,0cm)  node[above=1pt] {$(r,0)$}
          (0cm,1.15cm)  node[fill=white] {$(0,r)$};
    
    % origin
    \draw[fill=black] (0,0) circle(.013);

\end{tikzpicture}
	\caption{An agent's sight at a certain time step in an example environment. Two objects are within the agent's sight, 
    described by the vectors $o_{1}$ and $o_{2}$. However, the agents sensors will only receive information from the closest object.}
	\label{fig:2obj}
\end{figure}
\end{center}

\section{Control parameters and operation considerations}
In order to obtain the control values, we leverage two neural networks denoted $net_v$ and $net_{\beta}$, each one to process a single control parameter. 
$net_v$ is used to calculate the vector's length, $net_{\beta}$ the angle between $\overrightarrow{o}$ and the y-axis, respectively.
Both networks take 
vector $\overrightarrow{o}$ as input and provide a single output which will be used to control the agent. Instead of using a single network
for both values, we divide the task as there is no direct connection between the values to be computed. Furthermore, experimental results show that
the error of a single network for both calculations is much higher than the errors of the two networks combined. As the angle between
the vector and the y-axis is equal when we omit the sign, we will pass the input values to the neural network without taking the sign of $o_x$ (the x component
of the vector) into consideration. After obtaining the angle from network $net_{\beta}$, we add the sign again. This transformation is shown in figure \ref{fig:tf}
In addition, we scale the components of the vector to be $\in [0,1]$ which is done by dividing it by the radius of the semi circle (the range sight):

\begin{equation}
    o = \frac{o}{r}
\end{equation}

This will guarantee that $\overrightarrow{y_v}$ (Output of network $net_v$) is also $\in [0,1]$ and can therefore
be directly passed on as a motion command provided $\overrightarrow{y_v} > 0.2$. Scaling has to be done for the angle enclosed by the vector and the 
y-axis $\overrightarrow{y_\beta}$ (Output of network $net_\beta$), also. Therefore, we define the mapping

\begin{gather*} 
        f: [0,1] \rightarrow [0,\frac{\pi}{2}], \\ 
        n \mapsto \frac{\pi}{2} \cdot n
\end{gather*}

which takes care of the scaling from the network's output to the angular steering command in radians. Finally, we can state the equations used to
compute the control commands. The equation for $v_a$ now incorporates both scaling and the sign of object vector $\overrightarrow{o}$:

\begin{equation}
\begin{split}
	    v_a=\begin{cases}
        \overrightarrow{y_v}, \quad \text{if } \overrightarrow{y_v} > 0.2 \\
		0.2, \quad \text{otherwise} \\
	\end{cases}
    \\ \beta_a = sgn(o_x) \cdot -(f(\overrightarrow{y_{\beta}}))
\end{split}
\end{equation}

\begin{center}
\begin{figure}[H]

\begin{tikzpicture}[scale=4.5,cap=round,>=latex]
    % draw the coordinates
    \draw[step=0.1,black!20,thin] (0,0) grid (1cm,1cm);
    
    \node[] (origin) at (0,0) {}; 
    \draw[] (0.6cm,0.3cm) node[blue] {};
    \draw[blue] (0.6cm,0.35cm) node[] {$ o_1' = (\frac{3r}{5}\ \frac{3r}{10})$};
    \draw[blue, ->] (origin) -- (0.6cm,0.3cm);
    \draw[blue] (0.14cm,0.07cm) arc (0:106:0.11);
    \node[blue] (alpha) at (0.05,0.09) {$\beta$}; 
    
    \draw[->] (0cm,0cm) -- (1cm,0cm) node[right,fill=white] {$x$};
    \draw[->] (0cm,0cm) -- (0cm,1cm) node[above,fill=white] {$y$};
    
    % draw the unit circle
    \draw [black,thick,domain=0:90] plot ({cos(\x)}, {sin(\x)});
    
    % draw the horizontal and vertical coordinates
    % the placement is better this way
    \draw (1.15cm,0cm)  node[above=1pt] {$(r,0)$}
          (0cm,1.15cm)  node[fill=white] {$(0,r)$};
    
    % origin
    \draw[fill=black] (0,0) circle(.013);

\end{tikzpicture}
	\caption{Vector $o_1$ from figure \ref{fig:2obj} after transformation in a coordinate system with both $x,y \geq 0$. $o_1$ loses its sign and
    becomes $o_1'$. The control logic of the agent will later incorporate the sign.}
	\label{fig:tf}
\end{figure}
\end{center}

We can now state the equations we want our networks to approximate. The distance between agent and object is equal to the norm of the vector. Thus,
$net_v$ is to approximate:

\begin{equation}
    ||\overrightarrow{o}|| = sqrt(\overrightarrow{o_x}^2 + \overrightarrow{o_y}^2)
\end{equation}

\begin{equation}
    \beta = asin(\overrightarrow{o_x}^2 + \overrightarrow{o_y}^2)
\end{equation}

Figure \ref{fig:netstruc} shows the structure of networks $net_\beta$ and $net_v$. The size of both Input and output layer are equal to the amount
of components of the vectors. Thus, only the amount of hidden layer neurons can be subject to variation. The resulting network topology can be 
stated as topology form as $(2, n, 1)$. 
\begin{figure}[H]

\centering

\begin{tikzpicture}
\node [draw=black, align=center, circle] (i2) at (0,2) {}; 
\node [draw=black, align=center, circle] (i3) at (0,1) {}; 

\node [draw=black, align=center, circle] (h01) at (2,2.5) {}; 
\node [draw=black, align=center, circle] (h02) at (2,1.5) {};
\node [] (h03) at (2,0.5) {$\vdots$};

\node [draw=black, align=center, circle] (o1) at (4,1.5) {};

\draw[arrow]	(i2) -- (h01);
\draw[arrow]	(i2) -- (h02);
\draw[arrow]	(i2) -- (h03);

\draw[arrow]	(i3) -- (h01);
\draw[arrow]	(i3) -- (h02);
\draw[arrow]	(i3) -- (h03);

\draw[arrow]	(h01) -- (o1);

\draw[arrow]	(h02) -- (o1);

\draw[arrow]	(h03) -- (o1);

\node[draw=black,fit=(i2) (i3), ellipse] (inputLayer) {};
\node[draw=black,fit=(h01) (h02) (h03), ellipse] (hiddenLayer) {};
\node[draw=black,fit=(o1), ellipse] (outputLayer) {};

\node [align=center] (x) at (-1,1.5) {$\overrightarrow{x}$};

\node [align=center] (di) at (0,5) {\textcolor{black}Input\\ layer\\ $L_1$};
\node [align=center] (dh1) at (2,5) {\textcolor{black}Hidden\\ layer\\ $L_{2} = H$};
\node [align=center] (do) at (4,5) {\textcolor{black}Output\\ layer\\ $L_3$};

\node [align=center] (y1) at (5,1.5) {$\overrightarrow{y}$};

\end{tikzpicture}
\caption{Network structure of both ANNs $net_v$ and $net_\beta$. Its network topology is $(|L_1|=2, |H|=n,|L_3|=1)$}.
\label{fig:netstruc}
\end{figure}

\chapter{Evaluation}
\label{ch:eval}

\section{Supervised learning}

As discussed in the previous chapter, we applied two neural networks for each navigation task. Standard Backpropagation as introduced in \ref{ch:learning} has been used
to train each network with previously calculated data. An example of a training data file is shown in listing \ref{lst:exp}. As there is no way to predict the performance 
of different network structures, we trained the networks with a number of different configurations and compared the results afterwards. The decisive criteria for the
learning progression is the network error $E$ (Distance from network output to target values) which is to be minimised. In order to produce meaningful data, we created
a recent network average error, which averages over the last $100$ error values. This value can be observed to become smaller and smaller over time. 

\begin{lstlisting}[caption={An example of data used to train $net_v$ with Backpropagation.},label={lst:exp},stepnumber=1]
topology: 2 6 1
in:  0.204762060691 0.638586017483
out: 0.670611365265
in:  0.0167079901642 0.922020036351
out: 0.92217140726
in:  0.00612609545556 0.357105772792
out: 0.357158315047
\end{lstlisting}

\paragraph{Topology evaluation} We tested both networks with a MLN-structure $(2,n,1)$ as described in chapter \ref{ch:design} for a varying amount of hidden neurons.
Figure \ref{fig:hv} and \ref{fig:ha} show the results for a training data size of $n = 20,000$. Both an overall view as as well as a detailed view of the end of the training is given.
(For the sake of clarity, we omit certain configurations which provide no additional information e.g. show results very similar to the presented ones).
The results for $net_v$ in figure \ref{fig:hvo} clearly show how all network configurations allow for an overall error of $<0.075$, although, clear differences between individual 
configurations can be observed. First of all, we can observe that $|H| = 10$ requires the longest time to make noticeable progress. It takes around $2,000-2,500$ iterations for this 
particular configuration to finally lower its error below $0.1$. Another interesting point is $n=5,000$. Some topologies (e.g. (2,5,1) and (2,1,1) are unable to make any 
further learning progress and remain almost the same even after $15,000$ more iterations. However, 8 hidden units start to decrease once more. $n=10,000$ shows a similar point, where
4 hidden neurons allow for another error decrease, after topology $(2,3,1)$ started to decrease a bit earlier. After about $n=17,000-18,000$ an optimum seems to be reached. 
In the detailed view, we can see that topology $(2,8,1)$ provides the best approximation for the velocity control with a final error of around $2.086\%$ . 
It is therefore chosen for further experiments and the final application.

\begin{figure}[H]
\centering
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{files/supervised/hv.pdf}
  \caption{An overall view.}
  \label{fig:hvo}
\end{subfigure}%
\newline
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{files/supervised/hvend.pdf}
  \caption{A detailed few focused on the last training iterations.}
  \label{fig:hvend}
\end{subfigure}
\newline
\caption{Training results of $net_v$ for iteration size $n = 20,000$ and a 
         varying amount of hidden layer neurons $|H|$. $|H| = 5,9,7$ have 
         been left out due to their strong similarity with the shown results.}
\label{fig:hv}
\end{figure}

\begin{figure}[H]
\centering
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{files/supervised/ha.pdf}
  \caption{An overall view.}
  \label{fig:hao}
\end{subfigure}%
\newline
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{files/supervised/haend.pdf}
  \caption{A detailed few focused on the last training iterations.}
  \label{fig:haend}
\end{subfigure}
\newline
\caption{Training results of $net_\beta$ for iteration size $n = 20,000$ and a 
         varying amount of hidden layer neurons $|H|$. $|H| = 5,9,7$ have 
         been left out due to their strong similarity with the shown results.}
\label{fig:ha}
\end{figure}

\begin{figure}[H]
\centering
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{files/supervised/bv.pdf}
  \caption{$net_v$}
  \label{fig:ha}
\end{subfigure}%
\newline
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{files/supervised/ba.pdf}
  \caption{$net_\beta$}
  \label{fig:haend}
\end{subfigure}
\newline
\caption{Training results with and without bias neurons for both networks with
         the respective minimal topology as obtained from Figures \ref{fig:ha} and \ref{fig:hv}.}
\label{fig:hvend}
\end{figure}

Figure \ref{fig:action} shows trained agents in the 2D world. The grey semi circle above the agents head is the approximated sight range of each agent.
At time step $t=1$, several objects are located within the sight range of each agent, as indicated by the straight lines form the agent to the objects.
As mentioned in chapter ref{ch:design}, only the object closes to the agent will be used as sensor input. This object is indicated by a red line. Therefore
agent 1 (the one beneath the other) is to move in an almost straight line to collect the object. We can see that the agent successfully collected the targeted
object at time step $t=2$. In the meanwhile, the other agent has already collected 3 objects close to each other and is now aiming for one that was previously 
in the sight range of agent 1 at time step 1. $t=3$ shows that both agents have been able to once more collect the targeted objects and are now moving away 
from each other. 

\begin{figure}[H]
\centering
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=\linewidth]{files/action/1.png}
  \caption{time step $t=1$}
  \label{fig:a1}
\end{subfigure}%
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=\linewidth]{files/action/2.png}
  \caption{time step $t=2$}
  \label{fig:a2}
\end{subfigure}%
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=\linewidth]{files/action/3.png}
  \caption{time step $t=3$}
  \label{fig:a3}
\end{subfigure}
\caption{Demonstration of the successful supervised learning approach.
         Two agents are shown over three time steps.}
\label{fig:action}
\end{figure}

\chapter{Conclusion}
\label{ch:conclusion}
We have shown how ANNs in combination with supervised machine learning algorithms allow for a successful application on the
object collecting agent problem. Satisfactory behaviour can be achieved with a relatively small trainings dataset. The application of two neural
networks instead of merely one is vital for the successful application. The problem becomes a lot more complex when we restrict supervised learning. As 
agents are placed in the world within the world without any knowledge whatsoever, significantly longer training cycles must be expected. However, 
even with the application of the augmented genetic algorithm, behaviour is far from being as satisfactory as in the supervised case, let alone that
of an omniscient agent. Since we use the fitness function to optimise weights of each neural network, we incorporate randomness in the algorithm. 
Both agent and objects are placed at random, which might lead to a respectively low ranking of an agent with weights close to the optimum. This can 
happen in case of an unfortunate placement. This randomness in placement leads to a noisy signal. Therefore, it might be beneficial to find different means by which an agent is ranked and optimise that
function. Similar work can be found in \cite{DEEPMIND} where a single algorithm is used to train convolution neural networks to play a 
vast variety of Atari video games. In their paper, Mnih et al. stress the difficulty of applying their reinforcement learning approach to environments
encountered in video games. While supervised learning allows for the usage of large amounts of training data, both unsupervised and reinforcement
learning algorithms must be able to learn from a scalar value, often sparse, noisy and delayed. These are exactly the difficulties encountered in
the object collection task. A ranking of the agents fitness is only received at the end of each population and therefore after a great number of
individual decisions which might increase or lower the performance. At the end of each population, a single scalar value does not allow to reason 
about the quality of each of the thousands of individual decisions. In supervised learning, we provide a measure of error associated with each 
of the agent's decisions. Furthermore, the applications equal one another in terms of the dependency of behaviour decisions among themselves. 
An imperfect decision in one time step may result in situations where a number of correct decisions may not be able to make up for the first flawed 
one. Imagine an agent with a single object at the bottom left of its sight range. In case the agent chooses to steer in the opposite direction,
this object might fall out of sight. Since the agent does not posses memory of any kind, the object will be lost and might never be found again.
Thus, an increase in fitness which could have been easily obtained is missed. The same sort of dependencies is found in video games, where one
flawed decision might ruin a previously well-played game.

Changing network structure and learning algorithms according to these research results may allow for a successful 
application of unsupervised learning to the object collecting agent problem. 

\KOMAoptions{listof=leveldown}

\newpage

%=========================================LISTS=========================================

\listoffigures
\listoftables
\listofalgorithms
\lstlistoflistings

%=========================================DICTIONARY====================================

%dictiary style
\bibliographystyle{./files/alphadin}
%dictionary source
\bibliography{./files/bibdb}

\end{document}
